{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binom Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/gap/lib/python3.9/site-packages (1.26.0)\n",
      "Requirement already satisfied: torch in /opt/anaconda3/envs/gap/lib/python3.9/site-packages (2.2.2)\n",
      "Requirement already satisfied: tifffile in /opt/anaconda3/envs/gap/lib/python3.9/site-packages (2024.6.18)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/gap/lib/python3.9/site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/envs/gap/lib/python3.9/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/envs/gap/lib/python3.9/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/gap/lib/python3.9/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/gap/lib/python3.9/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/envs/gap/lib/python3.9/site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/gap/lib/python3.9/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/gap/lib/python3.9/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy torch tifffile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from HistBinomDataset import HistogramBinomDataset\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "CROP_SIZE = 256\n",
    "NUM_WORKERS = 4\n",
    "BINOM_SPLIT = 0.6\n",
    "\n",
    "# Paths\n",
    "ROOT_DIR = 'files'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = HistogramBinomDataset(\n",
    "    root_dir=ROOT_DIR,\n",
    "    crop_size=CROP_SIZE,\n",
    "    mode='train',\n",
    "    binomial_split=True,\n",
    "    binomial_prob=BINOM_SPLIT,          # Optional: adjust probability for binomial split\n",
    "    apply_augmentations=True,           # Enable D4 augmentations\n",
    "    virt_size=10000                     # Number of samples per epoch\n",
    ")\n",
    "\n",
    "test_dataset = HistogramBinomDataset(\n",
    "    root_dir=ROOT_DIR,\n",
    "    crop_size=CROP_SIZE,\n",
    "    mode='test',\n",
    "    binomial_split=False,\n",
    "    binomial_prob=BINOM_SPLIT,         # Optional: adjust probability for binomial split\n",
    "    apply_augmentations=False,         # Enable D4 augmentations\n",
    "    virt_size=100                      # Number of samples per epoch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    pin_memory=True,\n",
    "    num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    pin_memory=True,\n",
    "    num_workers=NUM_WORKERS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shape: torch.Size([32, 3, 17, 256, 256])\n",
      "Noisy shape: torch.Size([32, 3, 256, 256])\n",
      "Clean shape: torch.Size([32, 3, 256, 256])\n",
      "Train dataset size: 10000\n",
      "Test dataset size: 100\n"
     ]
    }
   ],
   "source": [
    "# Data Size !\n",
    "img = next(iter(test_loader))\n",
    "print(\"Batch shape:\", img['histogram'].shape)\n",
    "print(\"Noisy shape:\", img['noisy'].shape)\n",
    "print(\"Clean shape:\", img['clean'].shape)\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEURAL NETWORK !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONVOLUTION\n",
    "def conv3x3(in_channels, out_channels, stride=1, padding=1, bias=True):\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=padding, bias=bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownConv(nn.Module):\n",
    "    \"\"\"\n",
    "    Residual Down Convolution Block with optional max pooling.\n",
    "    3 conv layers with residual connection + pooling.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, pooling=True):\n",
    "        super(DownConv, self).__init__()\n",
    "        self.pooling = pooling\n",
    "        self.conv1 = conv3x3(in_channels, out_channels)\n",
    "        self.conv2 = conv3x3(out_channels, out_channels)\n",
    "        self.conv3 = conv3x3(out_channels, out_channels)\n",
    "        if self.pooling:\n",
    "            self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_skip = self.conv1(x)\n",
    "        x = F.relu(self.conv2(x_skip))\n",
    "        x = F.relu(self.conv3(x) + x_skip)  # residual add\n",
    "        before_pool = x\n",
    "        if self.pooling:\n",
    "            x = self.pool(x)\n",
    "        return x, before_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpConv(nn.Module):\n",
    "    \"\"\"\n",
    "    Residual Up Convolution Block.\n",
    "    Supports skip connection merge_mode: 'add' or 'concat'.\n",
    "    Upsamples with ConvTranspose2d.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, merge_mode='concat', up_mode='transpose'):\n",
    "        super(UpConv, self).__init__()\n",
    "        assert merge_mode in ('add', 'concat'), \"merge_mode must be 'add' or 'concat'\"\n",
    "        assert up_mode in ('transpose',), \"only 'transpose' up_mode supported for now\"\n",
    "\n",
    "        self.merge_mode = merge_mode\n",
    "\n",
    "        self.upconv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
    "\n",
    "        if self.merge_mode == 'concat':\n",
    "            self.conv1 = conv3x3(out_channels * 2, out_channels)\n",
    "        else:  # add\n",
    "            self.conv1 = conv3x3(out_channels, out_channels)\n",
    "\n",
    "        self.conv2 = conv3x3(out_channels, out_channels)\n",
    "        self.conv3 = conv3x3(out_channels, out_channels)\n",
    "\n",
    "    def forward(self, from_down, from_up):\n",
    "        from_up = self.upconv(from_up)\n",
    "        if self.merge_mode == 'concat':\n",
    "            x = torch.cat((from_up, from_down), dim=1)\n",
    "        else:  # add\n",
    "            x = from_up + from_down\n",
    "        x_skip = self.conv1(x)\n",
    "        x = F.relu(self.conv2(x_skip))\n",
    "        x = F.relu(self.conv3(x) + x_skip)  # residual add\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, n_bins=33, out_mode='mean', \n",
    "                 merge_mode='concat', depth=4, start_filters=64):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_channels: input channels (usually 3 for RGB)\n",
    "            n_bins: number of histogram bins\n",
    "            out_mode: 'mean' or 'distribution'\n",
    "            merge_mode: 'add' (residual) or 'concat' (default)\n",
    "            depth: number of downsampling layers (encoder depth)\n",
    "            start_filters: number of filters in first conv block\n",
    "        \"\"\"\n",
    "        super(UNet, self).__init__()\n",
    "        assert merge_mode in ('add', 'concat'), \"merge_mode must be 'add' or 'concat'\"\n",
    "\n",
    "        self.out_mode = out_mode\n",
    "        self.n_bins = n_bins\n",
    "        self.merge_mode = merge_mode\n",
    "        self.depth = depth\n",
    "        self.start_filters = start_filters\n",
    "\n",
    "        # Input channels after flattening histogram dimension\n",
    "        self.input_channels = in_channels * n_bins\n",
    "\n",
    "        # Encoder (DownConvs)\n",
    "        self.down_convs = nn.ModuleList()\n",
    "        in_ch = self.input_channels\n",
    "        for i in range(depth):\n",
    "            out_ch = start_filters * (2 ** i)\n",
    "            pooling = (i < depth - 1)\n",
    "            self.down_convs.append(DownConv(in_ch, out_ch, pooling=pooling))\n",
    "            in_ch = out_ch\n",
    "\n",
    "        # Decoder (UpConvs)\n",
    "        self.up_convs = nn.ModuleList()\n",
    "        for i in reversed(range(depth - 1)):\n",
    "            in_ch = start_filters * (2 ** (i + 1))\n",
    "            out_ch = start_filters * (2 ** i)\n",
    "            self.up_convs.append(UpConv(in_ch, out_ch, merge_mode=merge_mode))\n",
    "\n",
    "        # Final conv layer\n",
    "        if out_mode == 'mean':\n",
    "            self.final = nn.Conv2d(start_filters, 3, kernel_size=1)\n",
    "        elif out_mode == 'distribution':\n",
    "            self.final = nn.Conv2d(start_filters, 3 * n_bins, kernel_size=1)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid out_mode. Use 'mean' or 'distribution'.\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Input x: (B, 3, bins, H, W)\n",
    "        \"\"\"\n",
    "        B, C, bins, H, W = x.shape\n",
    "        x = x.view(B, C * bins, H, W)\n",
    "\n",
    "        encoder_outs = []\n",
    "\n",
    "        # Encoder path\n",
    "        for down in self.down_convs:\n",
    "            x, before_pool = down(x)\n",
    "            encoder_outs.append(before_pool)\n",
    "\n",
    "        # Decoder path\n",
    "        for i, up in enumerate(self.up_convs):\n",
    "            skip = encoder_outs[-(i + 2)]\n",
    "            x = up(skip, x)\n",
    "\n",
    "        # Final conv\n",
    "        out = self.final(x)\n",
    "\n",
    "        if self.out_mode == 'distribution':\n",
    "            out = out.view(B, 3, self.n_bins, H, W)\n",
    "            out = F.softmax(out, dim=2)  # softmax over bins\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using...  cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using... \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_from_histogram(hist):\n",
    "    # hist: (B, 3, bins, H, W)\n",
    "    device = hist.device\n",
    "    bins = hist.shape[2]\n",
    "    bin_centers = torch.linspace(0, 1, bins).to(device)  # assuming normalized bins [0,1]\n",
    "    # Multiply hist by bin centers and sum over bins axis to get mean\n",
    "    mean = torch.sum(hist * bin_centers.view(1, 1, bins, 1, 1), dim=2)  # shape (B, 3, H, W)\n",
    "    return mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, criterion, device=device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in loader:\n",
    "        hist = batch['histogram']  # tuple (h1, h2)\n",
    "\n",
    "        # Use h1 as input, h2 as target\n",
    "        input_hist = hist[0].to(device)\n",
    "        target_hist = hist[1].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output_mean = model(input_hist)                # output shape: (B, 3, H, W)\n",
    "        target_mean = compute_mean_from_histogram(target_hist)  # shape: (B, 3, H, W)\n",
    "\n",
    "        loss = criterion(output_mean, target_mean)    # criterion = nn.MSELoss()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNet(\n",
      "  (down_convs): ModuleList(\n",
      "    (0): DownConv(\n",
      "      (conv1): Conv2d(51, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (1): DownConv(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (2): DownConv(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (3): DownConv(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (up_convs): ModuleList(\n",
      "    (0): UpConv(\n",
      "      (upconv): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (1): UpConv(\n",
      "      (upconv): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (2): UpConv(\n",
      "      (upconv): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (final): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = UNet(\n",
    "    in_channels=3,\n",
    "    n_bins=17,\n",
    "    out_mode='mean',     # or 'distribution'\n",
    "    merge_mode='concat', # or 'add'\n",
    "    depth=4,\n",
    "    start_filters=64\n",
    ")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer & loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_epoch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/chiarapaglioni/Documents/GitHub/MCRT/N2N.ipynb Cell 18\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/chiarapaglioni/Documents/GitHub/MCRT/N2N.ipynb#X20sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m best_val_loss \u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(\u001b[39m'\u001b[39m\u001b[39minf\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/chiarapaglioni/Documents/GitHub/MCRT/N2N.ipynb#X20sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/chiarapaglioni/Documents/GitHub/MCRT/N2N.ipynb#X20sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     train_loss \u001b[39m=\u001b[39m train_epoch(model, train_loader, optimizer, criterion)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/chiarapaglioni/Documents/GitHub/MCRT/N2N.ipynb#X20sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mnum_epochs\u001b[39m}\u001b[39;00m\u001b[39m - Train Loss: \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/chiarapaglioni/Documents/GitHub/MCRT/N2N.ipynb#X20sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m torch\u001b[39m.\u001b[39msave(model\u001b[39m.\u001b[39mstate_dict(), \u001b[39m\"\u001b[39m\u001b[39mbest_unet.pth\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_epoch' is not defined"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, criterion)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "torch.save(model.state_dict(), \"best_unet.pth\")\n",
    "print(\"Saved best model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_and_plot(model, test_loader, device=device, num_samples=3):\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(test_loader):\n",
    "            if i >= num_samples:\n",
    "                break\n",
    "            \n",
    "            hist = batch['histogram']\n",
    "            input_hist = hist[0].to(device)\n",
    "            target_hist = hist[1].to(device)\n",
    "\n",
    "            # Model prediction (mean RGB)\n",
    "            pred_mean = model(input_hist)  # (B, 3, H, W)\n",
    "\n",
    "            # Compute input and target means from histograms\n",
    "            input_mean = compute_mean_from_histogram(input_hist)\n",
    "            target_mean = compute_mean_from_histogram(target_hist)\n",
    "\n",
    "            # Move to CPU and convert to numpy for plotting\n",
    "            input_mean_np = input_mean.cpu().numpy()\n",
    "            pred_mean_np = pred_mean.cpu().numpy()\n",
    "            target_mean_np = target_mean.cpu().numpy()\n",
    "\n",
    "            B = input_mean_np.shape[0]\n",
    "            for b in range(B):\n",
    "                fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
    "                axs[0].imshow(input_mean_np[b].transpose(1, 2, 0))\n",
    "                axs[0].set_title('Input Mean')\n",
    "                axs[0].axis('off')\n",
    "\n",
    "                axs[1].imshow(pred_mean_np[b].transpose(1, 2, 0))\n",
    "                axs[1].set_title('Predicted Mean')\n",
    "                axs[1].axis('off')\n",
    "\n",
    "                axs[2].imshow(target_mean_np[b].transpose(1, 2, 0))\n",
    "                axs[2].set_title('Target Mean')\n",
    "                axs[2].axis('off')\n",
    "\n",
    "                plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_and_plot(model, test_loader, device=device, num_samples=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
